{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-01 00:37:13--  https://dl.dropbox.com/s/p1ifzzf2443r05g/urbankiddo-c8528ce11ed0.json\n",
      "Resolving dl.dropbox.com... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to dl.dropbox.com|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://dl.dropboxusercontent.com/s/p1ifzzf2443r05g/urbankiddo-c8528ce11ed0.json [following]\n",
      "--2019-08-01 00:37:13--  https://dl.dropboxusercontent.com/s/p1ifzzf2443r05g/urbankiddo-c8528ce11ed0.json\n",
      "Resolving dl.dropboxusercontent.com... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to dl.dropboxusercontent.com|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2315 (2.3K) [text/plain]\n",
      "Saving to: ‘auth.json’\n",
      "\n",
      "auth.json           100%[===================>]   2.26K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-08-01 00:37:14 (460 MB/s) - ‘auth.json’ saved [2315/2315]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O \"auth.json\" https://dl.dropbox.com/s/p1ifzzf2443r05g/urbankiddo-c8528ce11ed0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gcloud\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/ab/d0cee58db2d8445c26e6f5db25d9b1f1aa14a3ab30eea8ce77ae808d10ef/gcloud-0.18.3.tar.gz (454kB)\n",
      "\u001b[K    100% |████████████████████████████████| 460kB 20.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting httplib2>=0.9.1 (from gcloud)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/55/3902b9f33ad9c15abf447ad91b86ef2d0835a1ae78530f1410c115cf8fe3/httplib2-0.13.1-py3-none-any.whl (94kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 23.2MB/s a 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos (from gcloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/ee/e59e74ecac678a14d6abefb9054f0bbcb318a6452a30df3776f133886d7d/googleapis-common-protos-1.6.0.tar.gz\n",
      "Collecting oauth2client>=2.0.1 (from gcloud)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/a9/4f25a14d23f0786b64875b91784607c2277eff25d48f915e39ff0cff505a/oauth2client-4.1.3-py2.py3-none-any.whl (98kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 33.7MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=3.0.0.b2.post1,>=3.0.0b2 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gcloud) (3.6.1)\n",
      "Requirement already satisfied: six in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gcloud) (1.11.0)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client>=2.0.1->gcloud)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/f0/b03e00ce9fddf4827c42df1c3ce10c74eadebfb706231e8d6d1c356a4062/pyasn1_modules-0.2.5-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 34.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1>=0.1.7 (from oauth2client>=2.0.1->gcloud)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/6e/209351ec34b7d7807342e2bb6ff8a96eef1fd5dcac13bdbadf065c2bb55c/pyasn1-0.4.6-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 30.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting rsa>=3.1.4 (from oauth2client>=2.0.1->gcloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /data/anaconda/envs/py35/lib/python3.5/site-packages (from protobuf!=3.0.0.b2.post1,>=3.0.0b2->gcloud) (40.2.0)\n",
      "Building wheels for collected packages: gcloud, googleapis-common-protos\n",
      "  Running setup.py bdist_wheel for gcloud ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/datascience/.cache/pip/wheels/b9/9b/9c/a01be401658fea33b93a35d03921b0c638266821b264dc8662\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/datascience/.cache/pip/wheels/9e/3d/a2/1bec8bb7db80ab3216dbc33092bb7ccd0debfb8ba42b5668d5\n",
      "Successfully built gcloud googleapis-common-protos\n",
      "\u001b[31mmxnet-model-server 1.0.1 requires model-archiver, which is not installed.\u001b[0m\n",
      "\u001b[31mchainermn 1.3.1 has requirement chainer<5.0,>=3.5.0, but you'll have chainer 5.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mflake8 3.7.5 has requirement pycodestyle<2.6.0,>=2.5.0, but you'll have pycodestyle 2.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-datalake-store 0.0.41 has requirement requests>=2.20.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mautovizwidget 0.12.7 has requirement plotly<3.0,>=1.10.0, but you'll have plotly 3.6.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.6.0 has requirement requests~=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.6.0 has requirement ruamel.yaml~=0.15.85, but you'll have ruamel-yaml 0.15.35 which is incompatible.\u001b[0m\n",
      "Installing collected packages: httplib2, googleapis-common-protos, pyasn1, pyasn1-modules, rsa, oauth2client, gcloud\n",
      "Successfully installed gcloud-0.18.3 googleapis-common-protos-1.6.0 httplib2-0.13.1 oauth2client-4.1.3 pyasn1-0.4.6 pyasn1-modules-0.2.5 rsa-4.0\n",
      "\u001b[33mYou are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting google-cloud-storage\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/e5/52cb018cac2fdb8942cbc70a3025a8fc6650863d0aee7f055af09248d783/google_cloud_storage-1.17.0-py2.py3-none-any.whl (66kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 15.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.0 (from google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/f0/084f598629db8e6ec3627688723875cdb03637acb6d86999bb105a71df64/google_cloud_core-1.0.3-py2.py3-none-any.whl\n",
      "Collecting google-auth>=1.2.0 (from google-cloud-storage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/9b/ed0516cc1f7609fb0217e3057ff4f0f9f3e3ce79a369c6af4a6c5ca25664/google_auth-1.6.3-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 19.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-resumable-media>=0.3.1 (from google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/5d/4bc5c28c252a62efe69ed1a1561da92bd5af8eca0cdcdf8e60354fae9b29/google_resumable_media-0.3.2-py2.py3-none-any.whl\n",
      "Collecting google-api-core<2.0.0dev,>=1.14.0 (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e5/7059475b3013a3c75abe35015c5761735ab224eb1b129fee7c8e376e7805/google_api_core-1.14.2-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 19.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa>=3.1.4 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from google-auth>=1.2.0->google-cloud-storage) (4.0)\n",
      "Collecting cachetools>=2.0.0 (from google-auth>=1.2.0->google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from google-auth>=1.2.0->google-cloud-storage) (1.11.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from google-auth>=1.2.0->google-cloud-storage) (0.2.5)\n",
      "Requirement already satisfied: pytz in /data/anaconda/envs/py35/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2018.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2.18.4)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (3.6.1)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (40.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from rsa>=3.1.4->google-auth>=1.2.0->google-cloud-storage) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mmxnet-model-server 1.0.1 requires model-archiver, which is not installed.\u001b[0m\n",
      "\u001b[31mazure-datalake-store 0.0.41 has requirement requests>=2.20.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mflake8 3.7.5 has requirement pycodestyle<2.6.0,>=2.5.0, but you'll have pycodestyle 2.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mchainermn 1.3.1 has requirement chainer<5.0,>=3.5.0, but you'll have chainer 5.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.6.0 has requirement requests~=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.6.0 has requirement ruamel.yaml~=0.15.85, but you'll have ruamel-yaml 0.15.35 which is incompatible.\u001b[0m\n",
      "\u001b[31mautovizwidget 0.12.7 has requirement plotly<3.0,>=1.10.0, but you'll have plotly 3.6.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: cachetools, google-auth, google-api-core, google-cloud-core, google-resumable-media, google-cloud-storage\n",
      "Successfully installed cachetools-3.1.1 google-api-core-1.14.2 google-auth-1.6.3 google-cloud-core-1.0.3 google-cloud-storage-1.17.0 google-resumable-media-0.3.2\n",
      "\u001b[33mYou are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Name: requests\n",
      "Version: 2.18.4\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: http://python-requests.org\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache 2.0\n",
      "Location: /data/anaconda/envs/py35/lib/python3.5/site-packages\n",
      "Requires: certifi, idna, chardet, urllib3\n",
      "Required-by: twine, Sphinx, sparkmagic, smart-open, requests-toolbelt, requests-oauthlib, requests-kerberos, requests-file, pydocumentdb, plotly, pandas-datareader, msrest, jupyterhub, h2o, gym, google-api-core, databricks-cli, blobxfer, azure-storage-common, azure-servicemanagement-legacy, azure-servicebus, azure-keyvault, azure-datalake-store, azure-cosmosdb-table, anaconda-project, anaconda-client, adal, mxnet\n",
      "\u001b[33mYou are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gcloud\n",
    "!pip install google-cloud-storage\n",
    "!pip show requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud\n",
      "  Downloading https://files.pythonhosted.org/packages/ba/b1/7c54d1950e7808df06642274e677dbcedba57f75307adf2e5ad8d39e5e0e/google_cloud-0.34.0-py2.py3-none-any.whl\n",
      "\u001b[31mmxnet-model-server 1.0.1 requires model-archiver, which is not installed.\u001b[0m\n",
      "\u001b[31mautovizwidget 0.12.7 has requirement plotly<3.0,>=1.10.0, but you'll have plotly 3.6.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mflake8 3.7.5 has requirement pycodestyle<2.6.0,>=2.5.0, but you'll have pycodestyle 2.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mchainermn 1.3.1 has requirement chainer<5.0,>=3.5.0, but you'll have chainer 5.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-datalake-store 0.0.41 has requirement requests>=2.20.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.6.0 has requirement requests~=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.6.0 has requirement ruamel.yaml~=0.15.85, but you'll have ruamel-yaml 0.15.35 which is incompatible.\u001b[0m\n",
      "Installing collected packages: google-cloud\n",
      "Successfully installed google-cloud-0.34.0\n",
      "\u001b[33mYou are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install google-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
      "Suggested packages:\n",
      "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
      "The following NEW packages will be installed:\n",
      "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
      "  chromium-codecs-ffmpeg-extra\n",
      "0 upgraded, 4 newly installed, 0 to remove and 88 not upgraded.\n",
      "Need to get 67.6 MB of archives.\n",
      "After this operation, 250 MB of additional disk space will be used.\n",
      "Get:1 http://azure.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 74.0.3729.169-0ubuntu0.16.04.1 [1,106 kB]\n",
      "Get:2 http://azure.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 chromium-browser amd64 74.0.3729.169-0ubuntu0.16.04.1 [59.3 MB]\n",
      "Get:3 http://azure.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 chromium-browser-l10n all 74.0.3729.169-0ubuntu0.16.04.1 [2,900 kB]\n",
      "Get:4 http://azure.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 chromium-chromedriver amd64 74.0.3729.169-0ubuntu0.16.04.1 [4,358 kB]\n",
      "Fetched 67.6 MB in 3s (19.1 MB/s)                   \u001b[0m3m\u001b[33m\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
      "(Reading database ... 432683 files and directories currently installed.)\n",
      "Preparing to unpack .../chromium-codecs-ffmpeg-extra_74.0.3729.169-0ubuntu0.16.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking chromium-codecs-ffmpeg-extra (74.0.3729.169-0ubuntu0.16.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [######....................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package chromium-browser.\n",
      "Preparing to unpack .../chromium-browser_74.0.3729.169-0ubuntu0.16.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking chromium-browser (74.0.3729.169-0ubuntu0.16.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [##############............................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package chromium-browser-l10n.\n",
      "Preparing to unpack .../chromium-browser-l10n_74.0.3729.169-0ubuntu0.16.04.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [####################......................................] \u001b8Unpacking chromium-browser-l10n (74.0.3729.169-0ubuntu0.16.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#######################...................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package chromium-chromedriver.\n",
      "Preparing to unpack .../chromium-chromedriver_74.0.3729.169-0ubuntu0.16.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [############################..............................] \u001b8Unpacking chromium-chromedriver (74.0.3729.169-0ubuntu0.16.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [##################################........................] \u001b8Processing triggers for gnome-menus (3.13.3-6ubuntu3.1) ...\n",
      "Processing triggers for desktop-file-utils (0.22-1ubuntu5.2) ...\n",
      "Processing triggers for mime-support (3.59ubuntu1) ...\n",
      "Processing triggers for man-db (2.7.5-1) ...\n",
      "Processing triggers for hicolor-icon-theme (0.15-0ubuntu1.1) ...\n",
      "Setting up chromium-codecs-ffmpeg-extra (74.0.3729.169-0ubuntu0.16.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up chromium-browser (74.0.3729.169-0ubuntu0.16.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [##########################################................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up chromium-browser-l10n (74.0.3729.169-0ubuntu0.16.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up chromium-chromedriver (74.0.3729.169-0ubuntu0.16.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [########################################################..] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[Jyes: standard output: Broken pipe\n",
      "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
      "Collecting selenium\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
      "\u001b[K    100% |████████████████████████████████| 911kB 18.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from selenium) (1.22)\n",
      "\u001b[31mmxnet-model-server 1.0.1 requires model-archiver, which is not installed.\u001b[0m\n",
      "\u001b[31mflake8 3.7.5 has requirement pycodestyle<2.6.0,>=2.5.0, but you'll have pycodestyle 2.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mautovizwidget 0.12.7 has requirement plotly<3.0,>=1.10.0, but you'll have plotly 3.6.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mazure-datalake-store 0.0.41 has requirement requests>=2.20.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mchainermn 1.3.1 has requirement chainer<5.0,>=3.5.0, but you'll have chainer 5.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.6.0 has requirement requests~=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.6.0 has requirement ruamel.yaml~=0.15.85, but you'll have ruamel-yaml 0.15.35 which is incompatible.\u001b[0m\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n",
      "\u001b[33mYou are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/garywu/google-compute-engine-selenium\n",
    "# # install chromium, its driver, and selenium\n",
    "!yes |sudo apt install chromium-chromedriver\n",
    "!sudo cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "!pip install selenium\n",
    "# set options to be headless, ..\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver= webdriver.Chrome('chromedriver',options=options)\n",
    "driver.quit()\n",
    "\n",
    "# open it, go to a website, and get results\n",
    "# wd = webdriver.Chrome('chromedriver',options=options)\n",
    "# wd.get(\"https://www.website.com\")\n",
    "# print(wd.page_source)  # results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>url</th>\n",
       "      <th>img_url</th>\n",
       "      <th>article_no</th>\n",
       "      <th>blob_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, brand, gender, category, size, price, url, img_url, article_no, blob_url]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "f = open(\"babyshop.csv\", \"w\", encoding='utf-8')\n",
    "writer = csv.DictWriter(f, fieldnames=[\"name\",\"brand\",\"gender\",\"category\",\"size\",\"price\",\"url\",\"img_url\",\"article_no\",\"blob_url\"])\n",
    "writer.writeheader()\n",
    "f.close()\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"babyshop.csv\", encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_bucket(blob_name, path_to_file):\n",
    "    \"\"\" Upload data to a bucket\"\"\" #doc string in function\n",
    "\n",
    "    # Explicitly use service account credentials by specifying the private key\n",
    "    # file.\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        'auth.json')\n",
    "\n",
    "    #print(buckets = list(storage_client.list_buckets())\n",
    "\n",
    "    bucket = storage_client.get_bucket('urbankiddo')\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(path_to_file)\n",
    "\n",
    "    #returns a public url\n",
    "    return blob.public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_list=['overdelar/s/629','trojor-stickat/s/1291','klanningar/s/631','underdelar/s/630','kjolar/s/643','shorts/s/646',\n",
    "        'kladset/s/1560','kostymer/s/1597','fleece/s/662','jackor/s/658','overaller/s/659','overdragsbyxor/s/1621',\n",
    "         'regnklader/s/661','sovklader/s/635','underklader/s/632','understall/s/663','uv-badklader/s/634','barnskor/s/620',\n",
    "         'accessoarer/s/637','babyprodukter/s/621','bilbarnstolar/s/623','barnvagnar/s/622'\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir babyshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import csv\n",
    "\n",
    "for cg in cg_list:\n",
    "    num=1\n",
    "    category=cg\n",
    "\n",
    "    main_url='https://www.babyshop.se/'+category+'?p=1056233&'\n",
    "\n",
    "    page_urls=main_url+'orderBy=Published&showAll=1'\n",
    "    gender='boy'\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.80 Safari/537.36'}\n",
    "    driver= webdriver.Chrome('chromedriver',options=options)\n",
    "\n",
    "    driver.get(page_urls)\n",
    "    driver.set_window_size(1920,1080)\n",
    "    \n",
    "    SCROLL_PAUSE_TIME = 3\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    \n",
    "    elems=driver.find_elements_by_xpath('//article[@class=\"product \"]')\n",
    "    print(len(elems))\n",
    "    for elem in elems:\n",
    "        url=elem.find_element_by_tag_name('a').get_attribute(\"href\")\n",
    "        html_doc=requests.get(url).content.decode()\n",
    "        soup=BeautifulSoup(html_doc, 'html.parser')\n",
    "        article_no=soup.find(\"li\",attrs={\"class\":\"item-no\"}).span.text\n",
    "        \n",
    "\n",
    "        \n",
    "        price=soup.find(\"del\").text\n",
    "        name=soup.find(\"h1\").text\n",
    "        brand=soup.find(\"h2\").text\n",
    "        img_url=soup.find(\"img\",attrs={\"class\":\"primary-image\"})['src']\n",
    "\n",
    "        \n",
    "        ac='babyshop'+'/'+article_no+str('.jpg')\n",
    "#             opener = urllib.request.URLopener()\n",
    "#             opener.addheader('User-Agent', 'whatever')\n",
    "#             filename, headers2 = opener.retrieve(img_url, ac)\n",
    "\n",
    "        rr = requests.get(img_url, allow_redirects=True)\n",
    "        open(ac, 'wb').write(rr.content)\n",
    "\n",
    "        blob_name=str('babyshop')+str('/')+str(article_no)\n",
    "        path_to_file='babyshop/'+article_no+str('.jpg')\n",
    "        blob_url=upload_to_bucket(blob_name, path_to_file)\n",
    "\n",
    "        for s in soup.find_all(\"option\")[1:]:\n",
    "            ls=s.text.strip().split('\\n')\n",
    "            size=ls[0]\n",
    "            price=ls[1]\n",
    "#             print(url,article_no,price,name,brand,img_url,size)\n",
    "            with open(\"babyshop.csv\", \"a\", encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([name,brand,gender,category,size,price,url,img_url,article_no,blob_url])\n",
    "\n",
    "        time.sleep(randint(3,20))\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588\n",
      "749\n",
      "1897\n",
      "912\n",
      "384\n",
      "281\n",
      "196\n",
      "2\n",
      "38\n",
      "443\n",
      "41\n",
      "5\n",
      "152\n",
      "105\n",
      "416\n",
      "28\n",
      "456\n",
      "1108\n",
      "536\n",
      "58\n",
      "0\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import csv\n",
    "\n",
    "for cg in cg_list:\n",
    "    num=1\n",
    "    category=cg\n",
    "\n",
    "    main_url='https://www.babyshop.se/'+category+'?p=1056246&'\n",
    "\n",
    "    page_urls=main_url+'orderBy=Published&showAll=1'\n",
    "    gender='girl'\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.80 Safari/537.36'}\n",
    "    driver= webdriver.Chrome('chromedriver',options=options)\n",
    "\n",
    "    driver.get(page_urls)\n",
    "    driver.set_window_size(1920,1080)\n",
    "    \n",
    "    SCROLL_PAUSE_TIME = 3\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    \n",
    "    elems=driver.find_elements_by_xpath('//article[@class=\"product \"]')\n",
    "    print(len(elems))\n",
    "    for elem in elems:\n",
    "        url=elem.find_element_by_tag_name('a').get_attribute(\"href\")\n",
    "        \n",
    "        html_doc=requests.get(url).content.decode()\n",
    "        soup=BeautifulSoup(html_doc, 'html.parser')\n",
    "        article_no=soup.find(\"li\",attrs={\"class\":\"item-no\"}).span.text\n",
    "        img_url=soup.find(\"img\",attrs={\"class\":\"primary-image\"})['src']\n",
    "\n",
    "        ac='babyshop'+'/'+article_no+str('.jpg')\n",
    "    #             opener = urllib.request.URLopener()\n",
    "    #             opener.addheader('User-Agent', 'whatever')\n",
    "    #             filename, headers2 = opener.retrieve(img_url, ac)\n",
    "\n",
    "        rr = requests.get(img_url, allow_redirects=True)\n",
    "        open(ac, 'wb').write(rr.content)\n",
    "\n",
    "        blob_name=str('babyshop')+str('/')+str(article_no)\n",
    "        path_to_file='babyshop/'+article_no+str('.jpg')\n",
    "        blob_url=upload_to_bucket(blob_name, path_to_file)\n",
    "        \n",
    "        \n",
    "        price=soup.find(\"del\").text\n",
    "        name=soup.find(\"h1\").text\n",
    "        brand=soup.find(\"h2\").text\n",
    "        for s in soup.find_all(\"option\")[1:]:\n",
    "            ls=s.text.strip().split('\\n')\n",
    "            size=ls[0]\n",
    "            price=ls[1]\n",
    "#             print(url,article_no,price,name,brand,img_url,size)\n",
    "            with open(\"babyshop.csv\", \"a\", encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([name,brand,gender,category,size,price,url,img_url,article_no,blob_url])\n",
    "\n",
    "#         time.sleep(randint(3,20))\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_list=['jackor/s/658','overaller/s/659','overdragsbyxor/s/1621',\n",
    "         'regnklader/s/661','sovklader/s/635','underklader/s/632','understall/s/663','uv-badklader/s/634','barnskor/s/620',\n",
    "         'accessoarer/s/637','babyprodukter/s/621','bilbarnstolar/s/623','barnvagnar/s/622'\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n",
      "140\n",
      "9\n",
      "336\n",
      "74\n",
      "419\n",
      "50\n",
      "104\n",
      "830\n",
      "1069\n",
      "1224\n",
      "300\n",
      "1087\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import csv\n",
    "\n",
    "for cg in cg_list:\n",
    "    num=1\n",
    "    category=cg\n",
    "\n",
    "    main_url='https://www.babyshop.se/'+category+'?p=44455&'\n",
    "\n",
    "    page_urls=main_url+'orderBy=Published&showAll=1'\n",
    "    gender='girl'\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.80 Safari/537.36'}\n",
    "    driver= webdriver.Chrome('chromedriver',options=options)\n",
    "\n",
    "    driver.get(page_urls)\n",
    "    driver.set_window_size(1920,1080)\n",
    "    \n",
    "    SCROLL_PAUSE_TIME = 3\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    \n",
    "    elems=driver.find_elements_by_xpath('//article[@class=\"product \"]')\n",
    "    print(len(elems))\n",
    "    for elem in elems:\n",
    "        try:\n",
    "            url=elem.find_element_by_tag_name('a').get_attribute(\"href\")\n",
    "            html_doc=requests.get(url).content.decode()\n",
    "            soup=BeautifulSoup(html_doc, 'html.parser')\n",
    "            article_no=soup.find(\"li\",attrs={\"class\":\"item-no\"}).span.text\n",
    "            img_url=soup.find(\"img\",attrs={\"class\":\"primary-image\"})['src']\n",
    "\n",
    "\n",
    "            ac='babyshop'+'/'+article_no+str('.jpg')\n",
    "        #             opener = urllib.request.URLopener()\n",
    "        #             opener.addheader('User-Agent', 'whatever')\n",
    "        #             filename, headers2 = opener.retrieve(img_url, ac)\n",
    "\n",
    "            rr = requests.get(img_url, allow_redirects=True)\n",
    "            open(ac, 'wb').write(rr.content)\n",
    "\n",
    "            blob_name=str('babyshop')+str('/')+str(article_no)\n",
    "            path_to_file='babyshop/'+article_no+str('.jpg')\n",
    "            blob_url=upload_to_bucket(blob_name, path_to_file)\n",
    "\n",
    "\n",
    "            price=soup.find(\"del\").text\n",
    "            name=soup.find(\"h1\").text\n",
    "            brand=soup.find(\"h2\").text\n",
    "            for s in soup.find_all(\"option\")[1:]:\n",
    "                ls=s.text.strip().split('\\n')\n",
    "                size=ls[0]\n",
    "                price=ls[1]\n",
    "    #             print(url,article_no,price,name,brand,img_url,size)\n",
    "                with open(\"babyshop.csv\", \"a\", encoding='utf-8') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([name,brand,gender,category,size,price,url,img_url,article_no,blob_url])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "#         time.sleep(randint(3,20))\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://storage.googleapis.com/urbankiddo/babyshop.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "storage_client = storage.Client.from_service_account_json('auth.json')\n",
    "bucket = storage_client.get_bucket('urbankiddo')\n",
    "blob = bucket.blob('babyshop.csv')\n",
    "blob.upload_from_filename('babyshop.csv')\n",
    "blob.public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
